{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks and LSTM In DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Recurrent Neural Networks/LSTM for Language Modeling\n",
    "\n",
    "Hello and welcome to this part. In this notebook, we will go over the topic of Language Modelling, and create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and benchmark on the Penn Treebank dataset. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objective\n",
    "\n",
    "By now, you should have an understanding of how Recurrent Networks work -- a specialized model to process sequential data by keeping track of the \"state\" or context. In this notebook, we go over a TensorFlow code snippet for creating a model focused on <b>Language Modelling</b> -- a very relevant task that is the cornerstone of many different linguistic problems such as <b>Speech Recognition, Machine Translation and Image Captioning</b>. For this, we will be using the Penn Treebank dataset, which is an often-used dataset for benchmarking Language Modelling models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###  What exactly is Language Modelling?\n",
    "\n",
    "Language Modelling, to put it simply, <b>is the task of assigning probabilities to sequences of words</b>. This means that, given a context of one or a sequence of words in the language the model was trained on, the model should provide the next most probable words or sequence of words that follows from the given sequence of words the sentence. Language Modelling is one of the most important tasks in Natural Language Processing.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png\" width=\"1080\">\n",
    "<center><i>Example of a sentence being predicted</i></center>\n",
    "\n",
    "In this example, one can see the predictions for the next word of a sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as <b>Speech Recognition, Machine Translation, Image Captioning, Text Correction</b> and many other very relevant problems. \n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png\" width=\"1080\">\n",
    "<center><i>The above example is a schema of an RNN in execution</i></center>\n",
    "\n",
    "As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach <b>low levels of perplexity</b> on our desired dataset.\n",
    "\n",
    "For Language Modelling problems, <b>perplexity</b> is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that <b>low perplexity means a higher degree of trust in the predictions the model makes</b>. Therefore, the lower perplexity is, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Penn Treebank dataset\n",
    "\n",
    "Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be able to be correct at all. This means that we need a <b>large amount of data, annotated by or at least corrected by humans</b>. This is, of course, not an easy task at all.\n",
    "\n",
    "The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is <i>huge</i> -- there are over <b>four million and eight hundred thousand</b> annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank is commonly used as a benchmark dataset for Language Modelling.\n",
    "\n",
    "The dataset is divided in different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag --<code>&lt;unk&gt;</code>\n",
    ", which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence.\n",
    "\n",
    "<center>Example of text from the dataset we are going to use, <b>ptb.train</b></center>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "The percentage of lung cancer deaths among the workers at the west <code>&lt;unk&gt;</code> mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said \n",
    " the plant which is owned by <code>&lt;unk&gt;</code> & <code>&lt;unk&gt;</code> co. was under contract with <code>&lt;unk&gt;</code> to make the cigarette filters \n",
    " the finding probably will support those who argue that the U.S. should regulate the class of asbestos including <code>&lt;unk&gt;</code> more <code>&lt;unk&gt;</code> than the common kind of asbestos <code>&lt;unk&gt;</code> found in most schools and other buildings dr. <code>&lt;unk&gt;</code> said\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "For better processing, in this example, we will make use of <a href=\"https://www.tensorflow.org/tutorials/word2vec/\"><b>word embeddings</b></a>, which is <b>a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers</b>. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be a very effective task in Natural Language Processing (NLP), and is a common place practice.\n",
    "<br><br>\n",
    "<font size=\"4\"><strong>\n",
    "$$Vec(\"Example\") = [0.02, 0.00, 0.00, 0.92, 0.30, \\ldots]$$\n",
    "</strong></font>\n",
    "<br>\n",
    "Word Embedding tends to group up similarly used words <i>reasonably</i> close together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and plot these words in a 2-dimensional space, we might see something like this:\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png\" width=\"800\">\n",
    "<center><i>T-SNE Mockup with clusters marked for easier visualization</i></center>\n",
    "<br><br>\n",
    "As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher they are correlated. For example, \"None\" is pretty semantically close to \"Zero\", while a phrase that uses \"Italy\", you could probably also fit \"Germany\" in it, with little damage to the sentence structure. The vectorial \"closeness\" for similar words like this is a great indicator of a well-built model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow.models.rnn</code></b> model, which includes the function for building RNNs, and <b><code>tensorflow.models.rnn.ptb.reader</code></b> which is the helper module for getting the input data from the dataset we just downloaded.\n",
    "\n",
    "If you want to learn more take a look at https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/ptb.zip\r\n",
      "   creating: data/ptb/\r\n",
      "  inflating: data/ptb/reader.py      \r\n",
      "   creating: data/__MACOSX/\r\n",
      "   creating: data/__MACOSX/ptb/\r\n",
      "  inflating: data/__MACOSX/ptb/._reader.py  \r\n",
      "  inflating: data/__MACOSX/._ptb     \r\n"
     ]
    }
   ],
   "source": [
    "# !mkdir data\n",
    "# !wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "# !unzip -o data/ptb.zip -d data\n",
    "# !cp data/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Building the LSTM model for Language Modeling\n",
    "\n",
    "Now that we know exactly what we are doing, we can start building our model using TensorFlow. The very first thing we need to do is download and extract the <code>simple-examples</code> dataset, which can be done by executing the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-27 14:48:45--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M  10.0KB/s    in 27m 7s  \n",
      "\n",
      "2020-03-27 15:16:00 (20.9 KB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "# !tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Additionally, for the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change those!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1 # Initial weight scale\n",
    "\n",
    "learning_rate = 1.0 # Initial learning rate\n",
    "\n",
    "max_grad_norm = 5 # Maximum permissible norm for the gradient \n",
    "#                 # (For gradient clipping -- another measure against Exploding Gradients)\n",
    "\n",
    "\n",
    "num_layers = 2    # The number of layers in our model\n",
    "\n",
    "num_steps = 20    # The total number of recurrence steps, also known as the number of layers \n",
    "                  # when our RNN is \"unfolded\"\n",
    "\n",
    "hidden_size_l1 = 256   # The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l2 = 128\n",
    "\n",
    "max_epoch_decay_lr = 4 # The maximum number of epochs trained with the initial learning rate\n",
    "\n",
    "max_epoch = 15         # The total number of epochs in training\n",
    "\n",
    "# The probability for keeping data in the Dropout Layer (This is an optimization, but is outside \n",
    "# our scope for this notebook!)\n",
    "keep_prob = 1          # At 1, we ignore the Dropout Layer wrapping.\n",
    "\n",
    "\n",
    "decay = 0.5            # The decay for the learning rate\n",
    "batch_size = 60        # The size for each batch of data\n",
    "\n",
    "vocab_size = 10000     # The size of our vocabulary\n",
    "embeding_vector_size = 200\n",
    "\n",
    "is_training = 1        # Training flag to separate training from testing\n",
    "\n",
    "data_dir = \"data/simple-examples/data/\" # Data directory for our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Some clarifications for LSTM architecture based on the arguments:\n",
    "\n",
    "Network structure:  \n",
    "\n",
    "- In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "- The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n",
    "  the structure is like:\n",
    "  - 200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output</li>\n",
    "\n",
    "Input layer:  \n",
    "\n",
    "- The network has 200 input units.</li>\n",
    "- Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n",
    "- The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n",
    "\n",
    "Hidden layer:\n",
    "\n",
    "- Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "There is a lot to be done and a ton of information to process at the same time, so go over this code slowly.  \n",
    "It may seem complex at first, but if you try to apply what you just learned about language modelling to the code you see, you should be able to understand it.\n",
    "\n",
    "This code is adapted from the <a href=\"https://github.com/tensorflow/models\">PTBModel</a> example bundled with the TensorFlow source code.\n",
    "\n",
    "\n",
    "**Train data**\n",
    "The story starts from data:\n",
    "\n",
    "- Train data is a list of words, of size 929,589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]\n",
    "\n",
    "- We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$\\lfloor \\frac{N}{b \\times h}\\rfloor+1=1548$$ iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, and h is size of each sentence.  \n",
    "  So, the number of iterators is 1548\n",
    "    \n",
    "- Each batch data is read from train dataset of size 600, and shape of [30x20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First we start an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0327 15:45:40.895340 140037471024960 module_wrapper.py:139] From /home/pascal/Projects/ML_DL/Notebooks/IBM_AI_Engineering/C05/w3/reader.py:30: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, w_id in word_to_id.items():\n",
    "            if w_id == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets just read one mini-batch now and feed our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets look at 3 sentences of our input x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n",
       "           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We define 2 place holders to feed them with mini-batchs, that is x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) # [60, 20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps])    # [60, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define a dictionary, and use it later to feed the placeholders with our first mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict = {_input_data:x, _targets:y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For example, we can use it to feed <code>\\_input\\_data</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, ..., 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361, ...,  241,   13, 2420],\n",
       "       [2654,    6,  334, ...,  514,    8,  605],\n",
       "       ...,\n",
       "       [7831,   36, 1678, ...,    4, 4558,  157],\n",
       "       [  59, 2070, 2433, ...,  400,    1, 1173],\n",
       "       [2097,    3,    2, ..., 2043,   23,    1]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0327 15:48:23.915415 140037471024960 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0327 15:48:24.312907 140037471024960 deprecation.py:323] From <ipython-input-17-ddf565d0e753>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0327 15:48:24.314121 140037471024960 deprecation.py:323] From <ipython-input-17-ddf565d0e753>:3: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, we initialize the states of the nework:\n",
    "\n",
    "**initial_state**\n",
    "\n",
    "For each LSTM, there are 2 state matrices, $c\\_state$ and $m\\_state$.  c_state and m_state represent \"Memory State\" and \"Cell State\".  \n",
    "Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size [30x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets look at the states, though they are all zero for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Embeddings\n",
    "We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values.  \n",
    "However, one-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient.  \n",
    "Thus, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n",
    "\n",
    "The embedded vectors also get updated during the training process of the deep neural network.\n",
    "We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of [10000x200] for all 10000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size]) # [10000 x 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the <code>embedding_words</code> variable with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01245048, -0.01313991,  0.01138183, ...,  0.00935629,\n",
       "         0.01207801,  0.01019577],\n",
       "       [-0.01912566, -0.00249455, -0.02132571, ...,  0.01311047,\n",
       "         0.02088102, -0.00369369],\n",
       "       [-0.01441067, -0.00224102, -0.01011756, ..., -0.00084317,\n",
       "        -0.00211873, -0.00167195],\n",
       "       ...,\n",
       "       [ 0.01570676, -0.01647092, -0.01499074, ...,  0.00942309,\n",
       "        -0.021896  , -0.02367462],\n",
       "       [ 0.00886982, -0.01331207, -0.02163364, ..., -0.0037888 ,\n",
       "         0.01506558, -0.01227487],\n",
       "       [ 0.02229507,  0.00113427, -0.01197676, ..., -0.01536694,\n",
       "        -0.0017784 ,  0.01088855]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic</code>.  \n",
    "It creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200. Each row of it is vector representing a word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(60, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  # shape=(60, 20, 200) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00111651, -0.00054782,  0.013181  , ...,  0.00364728,\n",
       "        -0.01168889,  0.01247218],\n",
       "       [-0.01469969,  0.00995582, -0.00974227, ...,  0.00511819,\n",
       "        -0.01908802, -0.00341777],\n",
       "       [-0.01291287, -0.01694011,  0.01787803, ..., -0.01572184,\n",
       "        -0.01559806,  0.00092852],\n",
       "       ...,\n",
       "       [-0.02198472, -0.01311403, -0.00124684, ..., -0.00805543,\n",
       "         0.00785643, -0.00946425],\n",
       "       [-0.00875028, -0.00908994, -0.00552665, ...,  0.01640131,\n",
       "         0.00566976, -0.0122617 ],\n",
       "       [ 0.00477075,  0.01416019, -0.00455836, ..., -0.01744168,\n",
       "         0.00284838, -0.00282374]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Constructing Recurrent Neural Networks\n",
    "\n",
    "**tf.nn.dynamic_rnn()** creates a recurrent neural network using **stacked_lstm**. \n",
    "\n",
    "The input should be a Tensor of shape: [batch_size, max_time, embedding_vector_size], in our case it would be (60, 20, 200)\n",
    "\n",
    "This method, returns a pair (outputs, new_state) where:\n",
    "- **outputs**: is a length T list of outputs (one for each input), or a nested tuple of such elements.\n",
    "- **new_state**: is the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0327 15:55:36.880361 140037471024960 deprecation.py:323] From <ipython-input-24-eb26e6ef7b2f>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0327 15:55:36.913459 140037471024960 deprecation.py:323] From /home/pascal/Projects/ML_DL/anaconda3/envs/tensorflow_keras_gpuenv/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "W0327 15:55:36.919208 140037471024960 deprecation.py:506] From /home/pascal/Projects/ML_DL/anaconda3/envs/tensorflow_keras_gpuenv/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "outputs, new_state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So, lets look at the outputs.  \n",
    "The output of the stackedLSTM comes from 200 hidden_layer, and in each time step (=20), one of them get activated.  \n",
    "We use the linear activation to map the 200 hidden layer to a [?x10 matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.91422921e-04,  2.79130327e-04, -1.03468374e-04, ...,\n",
       "        -4.51230608e-06,  4.60274932e-05,  2.36722321e-04],\n",
       "       [ 2.77096929e-04,  2.13577980e-04, -1.14699965e-03, ...,\n",
       "        -1.63189514e-04, -2.47793796e-04,  8.01820832e-04],\n",
       "       [ 3.27596907e-04,  1.93259693e-04, -1.90815167e-03, ...,\n",
       "        -5.39387693e-04, -2.73102778e-05,  7.46015285e-04],\n",
       "       ...,\n",
       "       [-9.34101932e-04,  6.83912833e-04,  1.71831289e-05, ...,\n",
       "        -1.43924131e-04,  7.55317742e-04, -2.02721290e-04],\n",
       "       [-1.15223741e-03,  2.76877719e-04,  1.60389231e-04, ...,\n",
       "         4.06843865e-05,  4.71413339e-04,  3.19448358e-04],\n",
       "       [-1.81414507e-04,  2.57129606e-04, -3.45288398e-04, ...,\n",
       "         4.53893736e-04,  1.97226560e-04,  8.06780241e-04]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We need to flatten the outputs to be able to connect it softmax layer. Let's reshape the output tensor from  [60 x 20 x 200] to [1200 x 200].\n",
    "\n",
    "**Notice:** Imagine our output is 3-d tensor as following (of course each <code>sen_x_word_y</code> is a an embedded vector by itself): \n",
    "- sentence 1: [[sen1word1], [sen1word2], [sen1word3], ..., [sen1word20]]\n",
    "- sentence 2: [[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20]]   \n",
    "- sentence 3: [[sen3word1], [sen3word2], [sen3word3], ..., [sen3word20]]  \n",
    "- ...\n",
    "- sentence 30: [[sen30word1], [sen30word2], [sen30word3], ..., [sen30word20]]\n",
    "\n",
    "Now, the flatten would convert this 3-dim tensor to:\n",
    "\n",
    "[ [sen1word1], [sen1word2], [sen1word3], ..., [sen1word20],[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20], ..., [sen30word20] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Logistic unit\n",
    "\n",
    "Now, we create a logistic unit to return the probability of the output word in our vocabulary with 10000 words. \n",
    "\n",
    "$$Softmax = [1200 \\times 128] * [128 \\times 10000] + [1 \\times 10000] \\Longrightarrow [1200 \\times 10000]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) # [128 x 10000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])                 # [1 x 10000]\n",
    "\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b                      # [1200 x 10000]\n",
    "prob = tf.nn.softmax(logits)                                           # [1200 x 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the probability of observing words for t=0 to t=20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output:  (1200, 10000)\n",
      "The probability of observing words in t=0 to t=20 [[1.00786536e-04 1.01101563e-04 9.88040993e-05 ... 1.00051984e-04\n",
      "  1.01243641e-04 1.00087491e-04]\n",
      " [1.00780264e-04 1.01106169e-04 9.88041065e-05 ... 1.00045087e-04\n",
      "  1.01250749e-04 1.00093275e-04]\n",
      " [1.00794343e-04 1.01099708e-04 9.87987733e-05 ... 1.00043173e-04\n",
      "  1.01254329e-04 1.00104800e-04]\n",
      " ...\n",
      " [1.00785182e-04 1.01098412e-04 9.88041065e-05 ... 1.00073150e-04\n",
      "  1.01250640e-04 1.00115991e-04]\n",
      " [1.00775753e-04 1.01100050e-04 9.87917592e-05 ... 1.00064986e-04\n",
      "  1.01261961e-04 1.00115394e-04]\n",
      " [1.00777565e-04 1.01093698e-04 9.87921740e-05 ... 1.00072197e-04\n",
      "  1.01261590e-04 1.00103214e-04]]\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "output_words_prob = session.run(prob, feed_dict)\n",
    "print(\"shape of the output: \", output_words_prob.shape)\n",
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Prediction\n",
    "\n",
    "What is the word correspond to the probability output? Lets use the maximum probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3541, 3541, 3541, 3541, 3541, 5899, 5899, 5899, 5899, 4685, 4685,\n",
       "        773, 1077, 1077, 7692, 7692, 7692,  305,  305, 1077])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0:20], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So, what is the ground truth for the first word of first sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, you can get it from target tensor, if you want to find the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = session.run(_targets, feed_dict) \n",
    "targ[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar the predicted words are to the target words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Objective function\n",
    "\n",
    "Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "This function is already implemented and available in TensorFlow through <b>sequence_loss_by_example</b>. It calculates the weighted cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n",
    "\n",
    "The arguments of this function are:  \n",
    "- logits: List of 2D Tensors of shape [batch_size $=1200$ x num_decoder_symbols $=10000$].  \n",
    "- targets: List of 1D batch-sized int32 Tensors of the same length as logits ($= 60 \\times 20 = 1200$)  \n",
    "- weights: List of 1D batch-sized float-Tensors of the same length as logits ($= 60$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], \n",
    "                                                          [tf.reshape(_targets, [-1])],\n",
    "                                                          [tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "loss is a 1D batch-sized float Tensor [1200x1]: The log-perplexity for each sequence. Lets look at the first 10 values of loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.196017, 9.197115, 9.210948, 9.220863, 9.215823, 9.219668,\n",
       "       9.200974, 9.20946 , 9.210599, 9.218319], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define loss as average of the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.18529"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "To do training for our network, we have to take the following steps:\n",
    "\n",
    "1. Define the optimizer.\n",
    "2. Extract variables that are trainable.\n",
    "3. Calculate the gradients based on the loss function.\n",
    "4. Apply the optimizer to the variables/gradients tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### 1. Define Optimizer\n",
    "\n",
    "*GradientDescentOptimizer* constructs a new gradient descent optimizer.  \n",
    "Later, we use constructed *optimizer* to compute gradients for a loss and apply gradients to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### 2. Trainable Variables\n",
    "\n",
    "Defining a variable, if you passed **trainable=True**, the variable constructor automatically adds new variables to the graph collection **GraphKeys.TRAINABLE_VARIABLES**.  \n",
    "Now, using *tf.trainable_variables()* you can get all variables created with **trainable=True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except lr, which we just created)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Note: we can find the name and scope of all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### 3. Calculate the gradients based on the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "##### Gradient\n",
    "\n",
    "The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.\n",
    "\n",
    "First let's recall the gradient function using an toy example: $ z = \\left(2x^2 + 3xy\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = tf.placeholder(tf.float32)\n",
    "var_y = tf.placeholder(tf.float32) \n",
    "\n",
    "func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(func_test, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The *tf.gradients()* function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors — including variables.  \n",
    "*tf.gradients(func, xs)* constructs symbolic partial derivatives of sum of *func* w.r.t. *x* in **xs**. \n",
    "\n",
    "Now, lets look at the derivative w.r.t. <b>var_x</b>: $ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 4x + 3y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_x])\n",
    "session.run(var_grad, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Te derivative w.r.t. <b>var_y</b>: $ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 3x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_y])\n",
    "session.run(var_grad, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we can look at gradients w.r.t all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7f5c5c3e2110>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/add_grad/Reshape:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "\n",
    "# sess.run(grad_t_list,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we have a list of tensors, t-list. We can use it to find clipped tensors.  \n",
    "**clip_by_global_norm** clips values of multiple tensors by the ratio of the sum of their norms.\n",
    "\n",
    "*clip_by_global_norm* gets *t-list* as input and returns 2 things:\n",
    "- a list of clipped tensors, so called *list_clipped*\n",
    "- the global norm (global_norm) of all tensors in t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0327 16:26:09.303615 140037471024960 deprecation.py:323] From /home/pascal/Projects/ML_DL/anaconda3/envs/tensorflow_keras_gpuenv/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7f5c5c438610>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedSlicesValue(values=array([[-1.5529071e-05,  7.6005426e-06, -6.1344799e-06, ...,\n",
       "          1.7288530e-05, -3.6976912e-06, -1.6721526e-05],\n",
       "        [-6.2620720e-06,  4.3404484e-06, -1.3176459e-06, ...,\n",
       "          5.8224614e-06, -1.5962712e-05, -1.1089724e-06],\n",
       "        [-4.1182179e-06,  3.8484404e-06, -7.6324250e-06, ...,\n",
       "          1.9853587e-06, -1.3936151e-05,  1.2161923e-05],\n",
       "        ...,\n",
       "        [-1.9177464e-06, -8.6266755e-07, -1.0473650e-06, ...,\n",
       "          4.1417675e-06,  5.7506954e-06, -1.4457864e-06],\n",
       "        [-4.0567570e-06, -8.7720764e-06,  2.9081707e-06, ...,\n",
       "          1.0367353e-06, -6.3475954e-06, -2.0716782e-06],\n",
       "        [ 3.6255665e-06, -3.4182260e-06,  5.3118065e-06, ...,\n",
       "         -3.9222332e-06, -1.0167446e-05,  2.2893853e-06]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1], dtype=int32), dense_shape=array([10000,   200], dtype=int32)),\n",
       " array([[ 4.67699195e-08,  5.41868737e-08, -1.30177852e-07, ...,\n",
       "         -1.58972444e-08,  3.09935331e-08, -9.84557236e-09],\n",
       "        [-2.54042121e-08, -1.24858683e-08,  3.78398397e-08, ...,\n",
       "          1.55990652e-08,  7.10691328e-09, -7.02731455e-08],\n",
       "        [ 1.37202010e-08,  6.48403908e-09,  7.06515948e-08, ...,\n",
       "         -4.34724701e-10, -1.66436216e-08,  4.34341949e-08],\n",
       "        ...,\n",
       "        [ 4.78593609e-09, -1.88796090e-09,  5.02107866e-09, ...,\n",
       "          1.15926895e-08,  1.63078739e-09, -6.99986114e-09],\n",
       "        [ 9.13452425e-09, -4.79762496e-09,  6.72592071e-09, ...,\n",
       "         -2.70447043e-09, -4.11931111e-09,  3.21989629e-10],\n",
       "        [ 5.09182652e-09, -2.62909428e-09, -1.22106647e-09, ...,\n",
       "          5.36795497e-10,  3.44525497e-09,  1.52752175e-08]], dtype=float32),\n",
       " array([ 4.5833622e-07,  2.5190416e-06, -1.5812817e-06, ...,\n",
       "        -3.8405781e-07,  5.7869090e-07,  8.8245253e-07], dtype=float32),\n",
       " array([[-2.57598831e-09, -4.92818986e-09, -6.00416916e-09, ...,\n",
       "         -9.10855924e-09,  1.51058916e-08, -1.40629497e-09],\n",
       "        [ 4.80827378e-10,  3.78695209e-09,  3.52949092e-09, ...,\n",
       "         -7.96568855e-09, -5.78700976e-11,  6.84481083e-11],\n",
       "        [-1.77950743e-09, -1.48847636e-08, -2.22071428e-09, ...,\n",
       "         -4.25948965e-09, -1.12193987e-08,  6.99156422e-09],\n",
       "        ...,\n",
       "        [-4.56515714e-10,  3.25359184e-11,  1.05300579e-09, ...,\n",
       "         -1.13143783e-09,  4.27929425e-09, -2.38340736e-10],\n",
       "        [-7.74319830e-10,  9.00335917e-10, -1.01811126e-09, ...,\n",
       "          3.83486665e-09, -5.71589753e-09,  2.28263031e-09],\n",
       "        [ 6.31345143e-10, -1.03450276e-10, -1.08925491e-09, ...,\n",
       "          1.11767255e-10, -3.47580464e-10,  2.99867042e-09]], dtype=float32),\n",
       " array([-2.46069067e-06,  9.27974543e-06,  2.26572092e-06, -1.85893919e-06,\n",
       "        -1.50833466e-06, -1.68750455e-06,  5.51071071e-06,  5.14864769e-06,\n",
       "        -1.52617645e-06,  7.88573743e-07,  1.16112324e-06, -9.02125237e-07,\n",
       "         6.45618229e-07, -2.01082344e-06, -6.69807548e-07,  5.23770836e-07,\n",
       "        -6.01304237e-06,  1.95369080e-06,  4.70879104e-06,  7.49297726e-07,\n",
       "        -3.78704158e-06,  4.66500614e-06, -7.54839562e-07, -2.92450068e-06,\n",
       "        -5.27394036e-07,  5.02337207e-06, -8.76351237e-07,  1.14874911e-05,\n",
       "        -6.59307034e-06, -3.44188857e-06, -8.51307050e-07, -2.98021382e-06,\n",
       "        -8.27477720e-07,  6.95237532e-07,  3.13638679e-06,  7.98402198e-06,\n",
       "        -5.46533374e-06, -2.75998855e-06,  2.65789112e-07,  7.68498830e-06,\n",
       "        -1.40765269e-06, -7.61234219e-07, -1.77238314e-06,  7.79648019e-07,\n",
       "        -2.61135870e-06,  1.64712458e-06,  2.97034330e-06, -1.04744850e-05,\n",
       "         2.07932817e-06,  1.20962659e-06,  1.02553099e-06,  8.82644599e-07,\n",
       "         3.11399413e-06, -1.07888063e-05, -3.43948955e-07, -3.28869248e-07,\n",
       "        -1.05488641e-06,  3.03800584e-06,  4.27274699e-06, -1.59808280e-07,\n",
       "         1.25403119e-06, -3.33098569e-06,  2.31397598e-06, -1.32961486e-06,\n",
       "        -1.85864576e-06, -1.10910878e-06, -3.48063668e-06,  4.51577398e-06,\n",
       "        -2.14023385e-06, -5.89319916e-06, -2.27007467e-06, -1.13273245e-06,\n",
       "         2.21740834e-07, -1.08350764e-06,  2.16470241e-07, -5.55217525e-07,\n",
       "         2.48131258e-08, -6.85664986e-07,  1.53832457e-06, -4.68559938e-06,\n",
       "         1.93006827e-06,  1.39776330e-06, -7.55219344e-06,  9.55446239e-07,\n",
       "         1.64030644e-06, -2.40598729e-06, -3.10008909e-06, -2.00656314e-06,\n",
       "        -3.12736438e-06,  9.19681224e-06,  3.54808049e-06,  5.04116724e-06,\n",
       "        -1.91823278e-06, -3.06439642e-06,  2.36354799e-06, -5.62179730e-06,\n",
       "         3.83795350e-06,  5.09153824e-06,  7.58062868e-07, -3.80201232e-06,\n",
       "         2.53542885e-06,  8.88388172e-07, -5.50439790e-06, -1.64500614e-06,\n",
       "         5.46653564e-06,  1.69877876e-06,  7.24519350e-06, -7.19428135e-06,\n",
       "        -8.35320861e-07,  8.09209439e-07,  5.23259700e-07, -1.59209071e-06,\n",
       "        -3.23419863e-06, -8.17798536e-07,  1.65494600e-06, -1.46116963e-06,\n",
       "        -2.13824660e-06,  6.64688878e-06, -3.77764741e-06, -7.02612124e-06,\n",
       "         3.56815735e-06,  1.40724637e-07,  6.29126134e-06, -1.57484237e-06,\n",
       "        -2.21750838e-06,  1.50098879e-06, -4.74877925e-06,  3.74742535e-06,\n",
       "        -4.77749575e-03,  1.65257193e-02,  1.97446309e-02,  6.00329274e-03,\n",
       "        -3.95008875e-03, -9.25444619e-05,  9.70382057e-03,  3.44962667e-04,\n",
       "        -1.59567595e-02, -2.73506204e-03, -2.49835830e-02,  1.27743287e-02,\n",
       "        -8.35127896e-04, -3.66444420e-03, -3.49084777e-03, -7.68355094e-03,\n",
       "        -1.89193115e-02, -1.69931855e-02, -2.18019374e-02, -1.28489044e-02,\n",
       "        -1.77153070e-02, -1.24762300e-02,  2.89553273e-02,  2.99223382e-02,\n",
       "        -9.35467985e-03, -3.23079601e-02,  2.59965882e-02,  1.41648995e-02,\n",
       "         2.34527234e-02, -1.48438280e-02, -9.25162900e-03, -6.00814866e-03,\n",
       "        -7.04113208e-03, -2.74255276e-02, -1.95295420e-02, -2.73290742e-02,\n",
       "        -1.24639068e-02, -2.09229849e-02,  9.77042038e-03,  1.50372703e-02,\n",
       "        -7.70316832e-03,  1.39915757e-02, -9.88497678e-03,  1.63544510e-02,\n",
       "         1.60329547e-02, -1.51101435e-02,  7.99544156e-03,  2.94711068e-02,\n",
       "        -1.17818434e-02,  2.69280095e-02, -1.74374096e-02, -9.29294247e-03,\n",
       "        -1.46081541e-02,  2.10261121e-02,  1.61908455e-02,  1.28450925e-02,\n",
       "         4.74800181e-04, -4.33058245e-03, -2.70689931e-02, -3.41494568e-03,\n",
       "         1.00877108e-02,  1.47086363e-02, -1.69983841e-02,  1.21495957e-02,\n",
       "        -3.09528201e-03, -2.94541102e-03, -4.08640830e-03,  3.81550228e-04,\n",
       "        -7.24226097e-03,  1.25077320e-02,  9.60849971e-03,  5.81980869e-03,\n",
       "        -2.33901143e-02, -4.67712991e-03,  1.12803606e-03,  1.13844462e-02,\n",
       "        -6.45697257e-03, -3.74315605e-02,  1.63578019e-02, -9.53553803e-03,\n",
       "        -7.19126221e-03, -6.49138878e-04, -1.89289469e-02,  4.04030131e-03,\n",
       "         8.91523249e-03, -2.42029354e-02,  5.74004417e-03,  3.22135456e-04,\n",
       "        -5.60950907e-03, -2.26545725e-02,  1.40602933e-02, -9.90451081e-04,\n",
       "         2.44553294e-03, -1.10148517e-02, -1.58765018e-02,  2.75710784e-03,\n",
       "         3.50905433e-02,  4.62810025e-02, -6.94399793e-03,  9.45814699e-03,\n",
       "         1.06127998e-02, -9.84512642e-03, -1.17355902e-02, -9.54787422e-04,\n",
       "        -1.04910554e-02,  1.79306064e-02, -3.52985524e-02, -1.30803604e-02,\n",
       "        -6.77782204e-03, -3.27693368e-03,  1.48869827e-02,  4.14395379e-03,\n",
       "        -1.09973354e-02, -3.95715889e-03, -1.51129290e-02,  2.40407772e-02,\n",
       "         2.89811310e-03, -2.44568363e-02, -2.29872912e-02,  1.62417814e-02,\n",
       "         9.40416567e-03, -4.04555677e-03,  5.68585098e-03, -8.83702282e-03,\n",
       "         1.30192218e-02, -3.79800145e-03, -1.17015336e-02,  1.38273034e-02,\n",
       "         1.01164449e-06,  5.60234548e-06,  2.55350096e-06, -4.40971462e-06,\n",
       "         2.92565164e-06, -1.17967511e-06,  3.82781491e-06,  4.61047057e-06,\n",
       "        -1.62263677e-06,  2.81385525e-07, -7.48070249e-07, -1.92081438e-06,\n",
       "         1.09106861e-06, -1.93729056e-06, -2.90855269e-06,  1.42400222e-06,\n",
       "        -6.74294961e-06,  8.05348350e-07,  8.88547504e-07,  3.40492306e-07,\n",
       "        -5.46137881e-06,  4.49718937e-06, -5.60497710e-07, -3.47599325e-06,\n",
       "        -3.79058321e-07,  5.27522025e-06,  4.04666935e-06,  6.02274304e-06,\n",
       "        -2.76434639e-06, -3.31562796e-06, -2.59822241e-06,  1.66227032e-08,\n",
       "        -3.57130784e-06,  1.97925579e-06, -1.81523569e-06,  4.89366175e-06,\n",
       "        -4.36783739e-06,  2.22939957e-06,  9.60342618e-07,  4.20375818e-06,\n",
       "         5.10423149e-07, -8.00120802e-07, -2.85977626e-06,  6.09326889e-07,\n",
       "         7.94253339e-08,  1.67688242e-07,  1.83749216e-06, -7.62588661e-06,\n",
       "         1.41129101e-06,  1.32670698e-06,  2.75162029e-06,  3.98327728e-07,\n",
       "         1.92598623e-06, -1.29571690e-05, -1.73609021e-06,  5.24222799e-07,\n",
       "        -1.25638371e-06, -1.13050760e-06,  2.74790591e-06, -1.06225821e-06,\n",
       "         5.15941423e-08, -3.55490874e-06,  4.36283472e-06,  9.93542358e-07,\n",
       "        -3.49767959e-07, -7.89738067e-07, -1.92980065e-06,  2.93891731e-07,\n",
       "        -1.52108771e-06, -6.00378189e-06, -1.11154588e-06,  1.50798576e-06,\n",
       "        -7.88277021e-07, -2.28523868e-06,  3.74183901e-06, -1.03144066e-06,\n",
       "        -1.70705528e-06, -1.67594249e-06,  8.04902527e-07, -3.25467272e-06,\n",
       "        -5.81358336e-07,  1.62108006e-06, -7.13085228e-06,  1.37020595e-06,\n",
       "         3.01743921e-06, -9.96332915e-07, -2.85600436e-07, -2.52101677e-06,\n",
       "        -1.25186807e-06,  3.93916343e-06,  1.51790778e-06,  3.71633837e-06,\n",
       "        -1.76379490e-06, -9.07126037e-07,  1.19291531e-06, -1.30356614e-06,\n",
       "         9.27951874e-07,  7.35221738e-06, -1.71703860e-07, -2.89034779e-06,\n",
       "         3.48756373e-07,  1.47642572e-06, -5.28221881e-06, -6.02752664e-07,\n",
       "         4.91255514e-06,  1.78645607e-07,  3.60917966e-06, -7.84505210e-06,\n",
       "        -1.72651710e-06,  6.01849536e-07, -1.69480120e-06, -2.15897171e-06,\n",
       "        -7.80920800e-07,  7.35134904e-07,  1.63013556e-06, -2.45484989e-06,\n",
       "        -3.07780965e-06,  3.65518713e-06, -3.17549620e-06, -4.06612435e-06,\n",
       "         4.46119338e-06, -6.05793105e-07,  2.23436268e-06,  4.29877787e-08,\n",
       "        -4.55882491e-06, -8.64977380e-07, -2.29511011e-06, -3.89047159e-08,\n",
       "        -2.46484274e-06,  9.28363806e-06,  2.26553243e-06, -1.86241471e-06,\n",
       "        -1.50471146e-06, -1.69096506e-06,  5.50549021e-06,  5.15930424e-06,\n",
       "        -1.52898326e-06,  7.87513386e-07,  1.16241756e-06, -9.01962380e-07,\n",
       "         6.45915861e-07, -2.00934051e-06, -6.69512872e-07,  5.21075208e-07,\n",
       "        -6.01445936e-06,  1.95209191e-06,  4.70863188e-06,  7.45233251e-07,\n",
       "        -3.78052027e-06,  4.67469226e-06, -7.55551696e-07, -2.92741629e-06,\n",
       "        -5.24417828e-07,  5.01399745e-06, -8.81513699e-07,  1.14953746e-05,\n",
       "        -6.59632451e-06, -3.43918509e-06, -8.49647108e-07, -2.98334749e-06,\n",
       "        -8.32548267e-07,  6.95598828e-07,  3.13809733e-06,  7.98135534e-06,\n",
       "        -5.46866886e-06, -2.76314631e-06,  2.64608190e-07,  7.68258997e-06,\n",
       "        -1.40608608e-06, -7.57807015e-07, -1.77027664e-06,  7.77562093e-07,\n",
       "        -2.60181469e-06,  1.64756352e-06,  2.97235260e-06, -1.04678747e-05,\n",
       "         2.07764856e-06,  1.20534878e-06,  1.02380147e-06,  8.82074119e-07,\n",
       "         3.11583813e-06, -1.07896367e-05, -3.44575000e-07, -3.25350101e-07,\n",
       "        -1.05577647e-06,  3.03729530e-06,  4.27035957e-06, -1.63220122e-07,\n",
       "         1.25743031e-06, -3.33428898e-06,  2.31130434e-06, -1.33477101e-06,\n",
       "        -1.85891054e-06, -1.10830888e-06, -3.47952937e-06,  4.51905407e-06,\n",
       "        -2.13665453e-06, -5.89429692e-06, -2.27607802e-06, -1.13155124e-06,\n",
       "         2.14330839e-07, -1.07779829e-06,  2.22091415e-07, -5.49523349e-07,\n",
       "         2.20966925e-08, -6.88697014e-07,  1.53747067e-06, -4.68187864e-06,\n",
       "         1.93445408e-06,  1.39213512e-06, -7.56102645e-06,  9.56512167e-07,\n",
       "         1.63926165e-06, -2.40430154e-06, -3.09937604e-06, -2.00883665e-06,\n",
       "        -3.12537009e-06,  9.19582908e-06,  3.54613894e-06,  5.03701631e-06,\n",
       "        -1.91537060e-06, -3.06720176e-06,  2.36611686e-06, -5.61931347e-06,\n",
       "         3.84440682e-06,  5.08582661e-06,  7.62564298e-07, -3.80670917e-06,\n",
       "         2.53387338e-06,  8.89615592e-07, -5.50016239e-06, -1.64517246e-06,\n",
       "         5.46932552e-06,  1.70293924e-06,  7.24107031e-06, -7.19433046e-06,\n",
       "        -8.30767249e-07,  8.06070375e-07,  5.22833034e-07, -1.58835610e-06,\n",
       "        -3.23637823e-06, -8.19853994e-07,  1.65570088e-06, -1.46030050e-06,\n",
       "        -2.12858777e-06,  6.64289428e-06, -3.77708511e-06, -7.02772331e-06,\n",
       "         3.57054364e-06,  1.40757408e-07,  6.29103943e-06, -1.58395983e-06,\n",
       "        -2.21661367e-06,  1.50017343e-06, -4.74905301e-06,  3.74752722e-06],\n",
       "       dtype=float32),\n",
       " array([[ 1.3101026e-05, -2.8002853e-05, -3.2655127e-05, ...,\n",
       "         -5.0553865e-08, -5.0460475e-08, -5.0743537e-08],\n",
       "        [-1.3680280e-04, -4.8642347e-04, -3.4822634e-04, ...,\n",
       "          5.3981796e-07,  5.3851932e-07,  5.4163240e-07],\n",
       "        [-1.3314083e-04, -2.9303614e-04, -8.9248366e-05, ...,\n",
       "          2.8775739e-07,  2.8707447e-07,  2.8873404e-07],\n",
       "        ...,\n",
       "        [ 6.5104185e-05,  2.1624977e-04,  6.0581471e-05, ...,\n",
       "         -2.4299445e-07, -2.4243050e-07, -2.4377835e-07],\n",
       "        [ 5.8205384e-05,  1.3811896e-05, -2.0349778e-05, ...,\n",
       "         -1.7128428e-07, -1.7088642e-07, -1.7191425e-07],\n",
       "        [ 8.9997433e-05,  1.7411361e-04,  2.8019117e-06, ...,\n",
       "          3.5905693e-08,  3.5786513e-08,  3.5999324e-08]], dtype=float32),\n",
       " array([-0.78131646, -1.0979841 , -0.9813466 , ...,  0.00199387,\n",
       "         0.00198904,  0.00200066], dtype=float32)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>4. Apply the optimizer to the variables / gradients tuple.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:  \n",
    "- We have to create the model in accordance with our defined hyperparameters\n",
    "- We have to create the placeholders for our input data and expected outputs (the real data)\n",
    "- We have to create the LSTM cell structure and connect them with our RNN structure\n",
    "- We have to create the word embeddings and point them to the input data\n",
    "- We have to create the input structure for our RNN\n",
    "- We have to instantiate our RNN model and retrieve the variable in which we should expect our outputs to appear\n",
    "- We need to create a logistic structure to return the probability of our words\n",
    "- We need to create the loss and cost functions for our optimizer to work, and then create the optimizer\n",
    "- And finally, we need to create a training operation that can be run to actually train our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, action_type):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) # [60, 20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])    # [60, 20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=256) of BasicLSTMCell is size of hidden layer, that is, the number\n",
    "        # of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations \n",
    "        # of the sentence.\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for \n",
    "        # our LSTM unit. This is an optimization of the LSTM output, but it is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function links the LSTM \n",
    "        # units to the RNN units. RNN cells are composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized \n",
    "        # with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size]) # [10000 x 200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for \n",
    "        # our inputs. This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, \n",
    "        # we create 30 word-vectors of size [30x2000]\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # \n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence\n",
    "        # at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are \n",
    "        # done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) # [200 x 1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # [1 x 1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        \n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################    \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        # Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\": return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, \n",
    "        # which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        \n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    ### Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n",
    "\n",
    "What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, \n",
    "# the data to be fed, and the operation to be run \n",
    "################################################################################\n",
    "\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "    # Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    # For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        # Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "        # Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        # Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %4d of %4d, perplexity: %5.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr   10 of  774, perplexity: 4093.263 speed: 32740 wps\n",
      "Itr   87 of  774, perplexity: 1269.214 speed: 40958 wps\n",
      "Itr  164 of  774, perplexity: 986.039 speed: 43096 wps\n",
      "Itr  241 of  774, perplexity: 825.675 speed: 43924 wps\n",
      "Itr  318 of  774, perplexity: 730.707 speed: 44428 wps\n",
      "Itr  395 of  774, perplexity: 651.929 speed: 44727 wps\n",
      "Itr  472 of  774, perplexity: 589.574 speed: 44968 wps\n",
      "Itr  549 of  774, perplexity: 534.502 speed: 45139 wps\n",
      "Itr  626 of  774, perplexity: 491.626 speed: 45218 wps\n",
      "Itr  703 of  774, perplexity: 456.854 speed: 45293 wps\n",
      "Epoch 1 : Train Perplexity: 432.222\n",
      "Epoch 1 : Valid Perplexity: 274.802\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr   10 of  774, perplexity: 277.498 speed: 38751 wps\n",
      "Itr   87 of  774, perplexity: 237.359 speed: 45163 wps\n",
      "Itr  164 of  774, perplexity: 227.611 speed: 44639 wps\n",
      "Itr  241 of  774, perplexity: 218.664 speed: 45184 wps\n",
      "Itr  318 of  774, perplexity: 216.100 speed: 45413 wps\n",
      "Itr  395 of  774, perplexity: 210.505 speed: 45531 wps\n",
      "Itr  472 of  774, perplexity: 206.247 speed: 45620 wps\n",
      "Itr  549 of  774, perplexity: 199.597 speed: 45701 wps\n",
      "Itr  626 of  774, perplexity: 194.046 speed: 45742 wps\n",
      "Itr  703 of  774, perplexity: 189.858 speed: 45773 wps\n",
      "Epoch 2 : Train Perplexity: 187.113\n",
      "Epoch 2 : Valid Perplexity: 182.271\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr   10 of  774, perplexity: 188.550 speed: 39429 wps\n",
      "Itr   87 of  774, perplexity: 161.475 speed: 45282 wps\n",
      "Itr  164 of  774, perplexity: 157.086 speed: 45658 wps\n",
      "Itr  241 of  774, perplexity: 152.436 speed: 45914 wps\n",
      "Itr  318 of  774, perplexity: 152.538 speed: 45933 wps\n",
      "Itr  395 of  774, perplexity: 149.623 speed: 45997 wps\n",
      "Itr  472 of  774, perplexity: 148.137 speed: 46002 wps\n",
      "Itr  549 of  774, perplexity: 144.459 speed: 45997 wps\n",
      "Itr  626 of  774, perplexity: 141.603 speed: 45999 wps\n",
      "Itr  703 of  774, perplexity: 139.754 speed: 45952 wps\n",
      "Epoch 3 : Train Perplexity: 138.723\n",
      "Epoch 3 : Valid Perplexity: 153.053\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr   10 of  774, perplexity: 150.387 speed: 39692 wps\n",
      "Itr   87 of  774, perplexity: 128.817 speed: 44291 wps\n",
      "Itr  164 of  774, perplexity: 126.219 speed: 43680 wps\n",
      "Itr  241 of  774, perplexity: 122.855 speed: 43697 wps\n",
      "Itr  318 of  774, perplexity: 123.627 speed: 44103 wps\n",
      "Itr  395 of  774, perplexity: 121.772 speed: 44336 wps\n",
      "Itr  472 of  774, perplexity: 120.993 speed: 44541 wps\n",
      "Itr  549 of  774, perplexity: 118.295 speed: 44747 wps\n",
      "Itr  626 of  774, perplexity: 116.311 speed: 44904 wps\n",
      "Itr  703 of  774, perplexity: 115.195 speed: 45046 wps\n",
      "Epoch 4 : Train Perplexity: 114.685\n",
      "Epoch 4 : Valid Perplexity: 141.407\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr   10 of  774, perplexity: 128.063 speed: 39101 wps\n",
      "Itr   87 of  774, perplexity: 110.218 speed: 45244 wps\n",
      "Itr  164 of  774, perplexity: 108.173 speed: 45603 wps\n",
      "Itr  241 of  774, perplexity: 105.578 speed: 45729 wps\n",
      "Itr  318 of  774, perplexity: 106.459 speed: 45799 wps\n",
      "Itr  395 of  774, perplexity: 105.010 speed: 45849 wps\n",
      "Itr  472 of  774, perplexity: 104.477 speed: 45898 wps\n",
      "Itr  549 of  774, perplexity: 102.292 speed: 45933 wps\n",
      "Itr  626 of  774, perplexity: 100.769 speed: 45926 wps\n",
      "Itr  703 of  774, perplexity: 99.964 speed: 45781 wps\n",
      "Epoch 5 : Train Perplexity: 99.721\n",
      "Epoch 5 : Valid Perplexity: 136.766\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr   10 of  774, perplexity: 110.196 speed: 33598 wps\n",
      "Itr   87 of  774, perplexity: 94.249 speed: 43210 wps\n",
      "Itr  164 of  774, perplexity: 91.957 speed: 44580 wps\n",
      "Itr  241 of  774, perplexity: 89.057 speed: 44429 wps\n",
      "Itr  318 of  774, perplexity: 89.336 speed: 44251 wps\n",
      "Itr  395 of  774, perplexity: 87.530 speed: 44051 wps\n",
      "Itr  472 of  774, perplexity: 86.670 speed: 43529 wps\n",
      "Itr  549 of  774, perplexity: 84.344 speed: 42862 wps\n",
      "Itr  626 of  774, perplexity: 82.669 speed: 42151 wps\n",
      "Itr  703 of  774, perplexity: 81.624 speed: 42358 wps\n",
      "Epoch 6 : Train Perplexity: 81.112\n",
      "Epoch 6 : Valid Perplexity: 127.669\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "Itr   10 of  774, perplexity: 96.252 speed: 38430 wps\n",
      "Itr   87 of  774, perplexity: 83.170 speed: 40268 wps\n",
      "Itr  164 of  774, perplexity: 81.415 speed: 41878 wps\n",
      "Itr  241 of  774, perplexity: 78.833 speed: 42986 wps\n",
      "Itr  318 of  774, perplexity: 79.119 speed: 43635 wps\n",
      "Itr  395 of  774, perplexity: 77.452 speed: 44101 wps\n",
      "Itr  472 of  774, perplexity: 76.580 speed: 44417 wps\n",
      "Itr  549 of  774, perplexity: 74.391 speed: 44677 wps\n",
      "Itr  626 of  774, perplexity: 72.772 speed: 44856 wps\n",
      "Itr  703 of  774, perplexity: 71.711 speed: 44966 wps\n",
      "Epoch 7 : Train Perplexity: 71.134\n",
      "Epoch 7 : Valid Perplexity: 124.924\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "Itr   10 of  774, perplexity: 88.753 speed: 38891 wps\n",
      "Itr   87 of  774, perplexity: 77.016 speed: 45172 wps\n",
      "Itr  164 of  774, perplexity: 75.506 speed: 45647 wps\n",
      "Itr  241 of  774, perplexity: 73.136 speed: 45782 wps\n",
      "Itr  318 of  774, perplexity: 73.429 speed: 45891 wps\n",
      "Itr  395 of  774, perplexity: 71.859 speed: 45400 wps\n",
      "Itr  472 of  774, perplexity: 71.033 speed: 45504 wps\n",
      "Itr  549 of  774, perplexity: 68.950 speed: 45538 wps\n",
      "Itr  626 of  774, perplexity: 67.398 speed: 45556 wps\n",
      "Itr  703 of  774, perplexity: 66.349 speed: 45618 wps\n",
      "Epoch 8 : Train Perplexity: 65.759\n",
      "Epoch 8 : Valid Perplexity: 124.063\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "Itr   10 of  774, perplexity: 84.443 speed: 39787 wps\n",
      "Itr   87 of  774, perplexity: 73.541 speed: 45433 wps\n",
      "Itr  164 of  774, perplexity: 72.188 speed: 45835 wps\n",
      "Itr  241 of  774, perplexity: 69.948 speed: 45931 wps\n",
      "Itr  318 of  774, perplexity: 70.267 speed: 45889 wps\n",
      "Itr  395 of  774, perplexity: 68.761 speed: 45885 wps\n",
      "Itr  472 of  774, perplexity: 67.971 speed: 45882 wps\n",
      "Itr  549 of  774, perplexity: 65.956 speed: 45925 wps\n",
      "Itr  626 of  774, perplexity: 64.454 speed: 45938 wps\n",
      "Itr  703 of  774, perplexity: 63.423 speed: 45955 wps\n",
      "Epoch 9 : Train Perplexity: 62.836\n",
      "Epoch 9 : Valid Perplexity: 123.724\n",
      "Epoch 10 : Learning rate: 0.031\n",
      "Itr   10 of  774, perplexity: 82.359 speed: 32445 wps\n",
      "Itr   87 of  774, perplexity: 71.710 speed: 41523 wps\n",
      "Itr  164 of  774, perplexity: 70.387 speed: 42154 wps\n",
      "Itr  241 of  774, perplexity: 68.218 speed: 42997 wps\n",
      "Itr  318 of  774, perplexity: 68.550 speed: 42794 wps\n",
      "Itr  395 of  774, perplexity: 67.079 speed: 43268 wps\n",
      "Itr  472 of  774, perplexity: 66.302 speed: 43708 wps\n",
      "Itr  549 of  774, perplexity: 64.326 speed: 43945 wps\n",
      "Itr  626 of  774, perplexity: 62.852 speed: 44203 wps\n",
      "Itr  703 of  774, perplexity: 61.837 speed: 44427 wps\n",
      "Epoch 10 : Train Perplexity: 61.253\n",
      "Epoch 10 : Valid Perplexity: 123.409\n",
      "Epoch 11 : Learning rate: 0.016\n",
      "Itr   10 of  774, perplexity: 81.313 speed: 39833 wps\n",
      "Itr   87 of  774, perplexity: 70.739 speed: 45257 wps\n",
      "Itr  164 of  774, perplexity: 69.413 speed: 45642 wps\n",
      "Itr  241 of  774, perplexity: 67.279 speed: 45815 wps\n",
      "Itr  318 of  774, perplexity: 67.617 speed: 45857 wps\n",
      "Itr  395 of  774, perplexity: 66.166 speed: 45964 wps\n",
      "Itr  472 of  774, perplexity: 65.397 speed: 45953 wps\n",
      "Itr  549 of  774, perplexity: 63.440 speed: 45952 wps\n",
      "Itr  626 of  774, perplexity: 61.978 speed: 45981 wps\n",
      "Itr  703 of  774, perplexity: 60.974 speed: 45978 wps\n",
      "Epoch 11 : Train Perplexity: 60.390\n",
      "Epoch 11 : Valid Perplexity: 123.159\n",
      "Epoch 12 : Learning rate: 0.008\n",
      "Itr   10 of  774, perplexity: 80.667 speed: 39241 wps\n",
      "Itr   87 of  774, perplexity: 70.188 speed: 45076 wps\n",
      "Itr  164 of  774, perplexity: 68.867 speed: 45641 wps\n",
      "Itr  241 of  774, perplexity: 66.752 speed: 45829 wps\n",
      "Itr  318 of  774, perplexity: 67.094 speed: 45820 wps\n",
      "Itr  395 of  774, perplexity: 65.655 speed: 45899 wps\n",
      "Itr  472 of  774, perplexity: 64.894 speed: 45909 wps\n",
      "Itr  549 of  774, perplexity: 62.949 speed: 45956 wps\n",
      "Itr  626 of  774, perplexity: 61.493 speed: 45960 wps\n",
      "Itr  703 of  774, perplexity: 60.498 speed: 45969 wps\n",
      "Epoch 12 : Train Perplexity: 59.913\n",
      "Epoch 12 : Valid Perplexity: 122.949\n",
      "Epoch 13 : Learning rate: 0.004\n",
      "Itr   10 of  774, perplexity: 80.260 speed: 39083 wps\n",
      "Itr   87 of  774, perplexity: 69.860 speed: 44069 wps\n",
      "Itr  164 of  774, perplexity: 68.555 speed: 44836 wps\n",
      "Itr  241 of  774, perplexity: 66.458 speed: 45178 wps\n",
      "Itr  318 of  774, perplexity: 66.804 speed: 45395 wps\n",
      "Itr  395 of  774, perplexity: 65.373 speed: 45516 wps\n",
      "Itr  472 of  774, perplexity: 64.616 speed: 45262 wps\n",
      "Itr  549 of  774, perplexity: 62.680 speed: 45399 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr  626 of  774, perplexity: 61.227 speed: 45479 wps\n",
      "Itr  703 of  774, perplexity: 60.237 speed: 45565 wps\n",
      "Epoch 13 : Train Perplexity: 59.652\n",
      "Epoch 13 : Valid Perplexity: 122.769\n",
      "Epoch 14 : Learning rate: 0.002\n",
      "Itr   10 of  774, perplexity: 80.030 speed: 39769 wps\n",
      "Itr   87 of  774, perplexity: 69.671 speed: 45125 wps\n",
      "Itr  164 of  774, perplexity: 68.377 speed: 45564 wps\n",
      "Itr  241 of  774, perplexity: 66.294 speed: 45733 wps\n",
      "Itr  318 of  774, perplexity: 66.643 speed: 45835 wps\n",
      "Itr  395 of  774, perplexity: 65.217 speed: 45861 wps\n",
      "Itr  472 of  774, perplexity: 64.464 speed: 45909 wps\n",
      "Itr  549 of  774, perplexity: 62.534 speed: 45922 wps\n",
      "Itr  626 of  774, perplexity: 61.083 speed: 45953 wps\n",
      "Itr  703 of  774, perplexity: 60.096 speed: 45709 wps\n",
      "Epoch 14 : Train Perplexity: 59.512\n",
      "Epoch 14 : Valid Perplexity: 122.644\n",
      "Epoch 15 : Learning rate: 0.001\n",
      "Itr   10 of  774, perplexity: 79.909 speed: 39364 wps\n",
      "Itr   87 of  774, perplexity: 69.567 speed: 44133 wps\n",
      "Itr  164 of  774, perplexity: 68.277 speed: 44698 wps\n",
      "Itr  241 of  774, perplexity: 66.202 speed: 44925 wps\n",
      "Itr  318 of  774, perplexity: 66.555 speed: 45072 wps\n",
      "Itr  395 of  774, perplexity: 65.132 speed: 45212 wps\n",
      "Itr  472 of  774, perplexity: 64.382 speed: 45359 wps\n",
      "Itr  549 of  774, perplexity: 62.456 speed: 45470 wps\n",
      "Itr  626 of  774, perplexity: 61.005 speed: 45564 wps\n",
      "Itr  703 of  774, perplexity: 60.020 speed: 45658 wps\n",
      "Epoch 15 : Train Perplexity: 59.437\n",
      "Epoch 15 : Valid Perplexity: 122.575\n",
      "Test Perplexity: 117.468\n"
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, \n",
    "    # they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    # Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower Perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This is the end of the <b>Applying Recurrent Neural Networks to Text Processing</b> notebook. Hopefully you now have a better understanding of Recurrent Neural Networks and how to implement one utilizing TensorFlow. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. __PowerAI__ speeds up deep learning and AI. Built on IBM’s Power Systems, __PowerAI__ is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The __PowerAI__ platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IMB Cloud](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use __Watson Studio__ to run these notebooks faster with bigger datasets.__Watson Studio__ is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, __Watson Studio__ enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of __Watson Studio__ users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thanks for completing this lesson!\n",
    "\n",
    "Notebook created by <a href=\"https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121\">Walter Gomes de Amorim Junior</a>, <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
