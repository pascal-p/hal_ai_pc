{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the final project of “Apache Spark for Scalable Machine Learning on BigData”. In this assignment you’ll analyze a real-world dataset and apply machine learning on it using Apache Spark. \n",
    "\n",
    "In order to pass, you need to implement some code (as described in the instruction section on Coursera) and finally answer a quiz on the Coursera platform.\n",
    "\n",
    "Let’s start by downloading the dataset and creating a dataframe. This dataset can be found on DAX, the IBM Data Asset Exchange and can be downloaded for free.\n",
    "\n",
    "https://developer.ibm.com/exchanges/data/all/jfk-weather-data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://turing.corto-nz.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faf047ed210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-20 12:09:09--  http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n",
      "Resolving max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2575759 (2.5M) [application/x-tar]\n",
      "Saving to: ‘jfk_weather.tar.gz’\n",
      "\n",
      "jfk_weather.tar.gz  100%[===================>]   2.46M  1.55MB/s    in 1.6s    \n",
      "\n",
      "2020-03-20 12:09:12 (1.55 MB/s) - ‘jfk_weather.tar.gz’ saved [2575759/2575759]\n",
      "\n",
      "./._jfk_weather.csv\n",
      "jfk_weather.csv\n"
     ]
    }
   ],
   "source": [
    "# delete files from previous runs\n",
    "!rm -f jfk_weather*\n",
    "\n",
    "# download the file containing the data in CSV format\n",
    "!wget http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n",
    "\n",
    "# extract the data\n",
    "!tar xvfz jfk_weather.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe out of it by using the first row as field names and trying to infer a schema based\n",
    "# on contents\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('jfk_weather.csv')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains some null values, therefore schema inference doesn’t work properly for all columns, in addition, a column contained trailing characters, so we need to clean up the data set first.  \n",
    "This is a normal task in any data science project since your data is never clean, don’t worry if you don’t understand all code, you won’t be asked about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from pyspark.sql.functions import translate, col\n",
    "\n",
    "df_cleaned = df \\\n",
    "    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n",
    "    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n",
    "    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n",
    "    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n",
    "    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n",
    "    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\"))\n",
    "\n",
    "df_cleaned = df_cleaned \\\n",
    "                .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n",
    "                .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n",
    "                .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n",
    "                .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double'))\n",
    "\n",
    "df_filtered = df_cleaned.filter(\"\"\"\n",
    "    HOURLYWindSpeed <> 0\n",
    "    and HOURLYWindSpeed IS NOT NULL\n",
    "    and HOURLYWindDirection <> 0\n",
    "    and HOURLYWindDirection IS NOT NULL\n",
    "    and HOURLYStationPressure <> 0\n",
    "    and HOURLYStationPressure IS NOT NULL\n",
    "    and HOURLYPressureTendency <> 0\n",
    "    and HOURLYPressureTendency IS NOT NULL\n",
    "    and HOURLYPressureTendency <> 0\n",
    "    and HOURLYPressureTendency IS NOT NULL\n",
    "    and HOURLYPrecip <> 0\n",
    "    and HOURLYPrecip IS NOT NULL\n",
    "    and HOURLYRelativeHumidity <> 0\n",
    "    and HOURLYRelativeHumidity IS NOT NULL\n",
    "    and HOURLYDRYBULBTEMPC <> 0\n",
    "    and HOURLYDRYBULBTEMPC IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the value of one column based of some others. It is sometimes helpful to print a correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.06306013, -0.4204518 ],\n",
       "       [ 0.06306013,  1.        , -0.19199348],\n",
       "       [-0.4204518 , -0.19199348,  1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\", ],\n",
    "                                  outputCol=\"features\")\n",
    "df_pipeline = vectorAssembler.transform(df_filtered)\n",
    "\n",
    "\n",
    "#\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "Correlation.corr(df_pipeline, \"features\").head()[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.06432947],\n",
       "       [0.06432947, 1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Course Project Quiz - Q1\n",
    "vectorAssembler_Q1 = VectorAssembler(inputCols=[\"HOURLYWindSpeed\", \"HOURLYPressureTendency\"],\n",
    "                                  outputCol=\"features\")\n",
    "df_pipeline_Q1 = vectorAssembler_Q1.transform(df_filtered)\n",
    "\n",
    "\n",
    "#\n",
    "# from pyspark.ml.stat import Correlation\n",
    "Correlation.corr(df_pipeline_Q1, \"features\").head()[0].toArray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see:  \n",
    "*HOURLYWindSpeed* and *HOURLYWindDirection* correlate with 0.06306013 whereas  \n",
    "*HOURLYWindSpeed*  and *HOURLYStationPressure* correlate with -0.4204518,  \n",
    "this is a good sign if we want to predict *HOURLYWindSpeed* from *HOURLYWindDirection* and *HOURLYStationPressure*.\n",
    "\n",
    "Since this is supervised learning, let’s split our data into train (80%) and test (20%) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df_filtered.randomSplit([0.8, 0.2])\n",
    "\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can re-use our feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\n",
    "  \"HOURLYWindDirection\",\n",
    "  \"ELEVATION\",\n",
    "  \"HOURLYStationPressure\"], outputCol=\"features\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function for evaluating our regression prediction performance.  \n",
    "We’re using RMSE (Root Mean Squared Error) here, the smaller the better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def regression_metrics(prediction):        \n",
    "    evaluator = RegressionEvaluator(\n",
    "    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(prediction)\n",
    "    print(\"RMSE on test data = %g\" % rmse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run a linear regression model first for building a baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 6.03474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0347415133058835"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LR1\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features', \n",
    "                      maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer, lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "\n",
    "regression_metrics(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 1.03543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.03543219955495"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Course Project Quiz - Q3\n",
    "# Please change #LR1 in order to use features_norm over features. What's the RMSE value you get now?\n",
    "lr_Q3 = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features_norm', \n",
    "                      maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
    "\n",
    "\n",
    "pipeline_Q3 = Pipeline(stages=[vectorAssembler, normalizer, lr_Q3])\n",
    "model_Q3 = pipeline_Q3.fit(df_train)\n",
    "prediction_Q3 = model_Q3.transform(df_test)\n",
    "\n",
    "regression_metrics(prediction_Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll try a Gradient Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 6.588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.58800459944986"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBT1\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer, gbt])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "\n",
    "regression_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s switch gears. Previously, we tried to predict HOURLYWindSpeed, but now we want predict HOURLYWindDirection.  \n",
    "In order to turn this into a classification problem we discretize the value using the Bucketizer. The new feature is called HOURLYWindDirectionBucketized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "\n",
    "bucketizer = Bucketizer(splits=[0, 180, float('Inf') ], \n",
    "                        inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a function in order to assess how we perform. Here we just use the accuracy measure which gives us the fraction of correctly classified examples.  \n",
    "Here, 0 is bad, 1 is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def classification_metrics(prediction):    \n",
    "    mcEval = MulticlassClassificationEvaluator()\\\n",
    "      .setMetricName(\"accuracy\")\\\n",
    "      .setPredictionCol(\"prediction\")\\\n",
    "      .setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
    "    accuracy = mcEval.evaluate(prediction)\n",
    "    print(\"Accuracy on test data = %g\" % accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for baselining we use LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.595318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5953177257525084"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LGReg1 - LogRegr\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n",
    "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer, vectorAssembler, normalizer, lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "\n",
    "classification_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try some other Algorithms and see if model performance increases.  \n",
    "It’s also important to tweak other parameters like parameters of individual algorithms (e.g. number of trees for RandomForest) or parameters in the feature engineering pipeline, e.g. train/test split ratio, normalization, bucketing, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.658863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6588628762541806"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF1\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=30)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\", \"HOURLYDRYBULBTEMPC\", \n",
    "                                             \"ELEVATION\", \"HOURLYStationPressure\",\n",
    "                                             \"HOURLYPressureTendency\", \"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "\n",
    "classification_metrics(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.648829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6488294314381271"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Course Project Quiz - Q8\n",
    "# If you change the number of trees in cell #RF1 from 30 to 10, what's the new accuracy?\n",
    "\n",
    "rf_Q08 = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=10)\n",
    "\n",
    "vectorAssembler_Q08 = VectorAssembler(inputCols=[\"HOURLYWindSpeed\", \"HOURLYDRYBULBTEMPC\", \n",
    "                                             \"ELEVATION\", \"HOURLYStationPressure\",\n",
    "                                             \"HOURLYPressureTendency\", \"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "\n",
    "pipeline_Q08 = Pipeline(stages=[bucketizer, vectorAssembler, normalizer, rf_Q08])\n",
    "model_Q08 = pipeline_Q08.fit(df_train)\n",
    "prediction_Q08 = model_Q08.transform(df_test)\n",
    "\n",
    "classification_metrics(prediction_Q08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.64214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6421404682274248"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBT2\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\", \"HOURLYDRYBULBTEMPC\",\n",
    "                                             \"ELEVATION\", \"HOURLYStationPressure\",\n",
    "                                             \"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\n",
    "\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_test)\n",
    "\n",
    "classification_metrics(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
